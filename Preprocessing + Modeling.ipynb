{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "EDA + Preprocessing + Modeling",
      "provenance": [],
      "collapsed_sections": [
        "g82sWTvXYyR0",
        "O3AciHiYpAD1",
        "ahyd9brWZess",
        "_lhSR_xR63mh",
        "Yq3DZBRekaRL",
        "arCjOqoikmhG",
        "UW8_mcxG3rN2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/5502-DataMining/CovidSentiment/blob/main/Preprocessing%20%2B%20Modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g82sWTvXYyR0"
      },
      "source": [
        "## **Preprocessing and Modeling**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yju-0HCdZZsQ",
        "collapsed": true
      },
      "source": [
        "from wordcloud import WordCloud\n",
        "import re\n",
        "import unicodedata\n",
        "import nltk\n",
        "import csv\n",
        "import string\n",
        "from textblob import TextBlob\n",
        "from nltk.corpus import stopwords\n",
        "import demoji\n",
        "\n",
        "import transformers\n",
        "from transformers import BertTokenizer, BertModel, BertConfig, AdamW, get_linear_schedule_with_warmup\n",
        "import torch\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from pylab import rcParams\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from collections import defaultdict\n",
        "from textwrap import wrap\n",
        "\n",
        "import pickle\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "SEED = 1024\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "from torch.utils.data import TensorDataset, DataLoader, Dataset, RandomSampler, SequentialSampler\n",
        "import torchtext\n",
        "from torchtext import data\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import operator\n",
        "from sklearn.metrics import hamming_loss, jaccard_score, label_ranking_average_precision_score, f1_score\n",
        "from tqdm._tqdm_notebook import tqdm_notebook as tqdm\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'\n",
        "\n",
        "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
        "\n",
        "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
        "\n",
        "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
        "\n",
        "rcParams['figure.figsize'] = 12, 8\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device\n",
        "\n",
        "nltk.download('all')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9lC6lhSdkb3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d6c06f0-92b8-49a8-9b7d-03c6382deac6"
      },
      "source": [
        "!git clone https://github.com/5502-DataMining/CovidSentiment/"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'CovidSentiment' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFrdFnVre788"
      },
      "source": [
        "demoji.download_codes()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMAci_v5gjUi"
      },
      "source": [
        "senwave = pd.read_csv(\"/content/COVID19_sentinentanalysissocialmedia/labeledEn.csv\")\n",
        "print(\"Length of Senwave Dataset = {}\".format(len(senwave)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMo48wKCh1TL"
      },
      "source": [
        "senwave.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3AciHiYpAD1"
      },
      "source": [
        "# **Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hD26IRPI_dzI"
      },
      "source": [
        "contractions = { \n",
        "\"ain't\": \"am not\", \"aren't\": \"are not\", \"can't\": \"cannot\", \"can't've\": \"cannot have\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
        "\"couldn't've\": \"could not have\", \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hadn't've\": \"had not have\",\n",
        "\"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he had\", \"he'd've\": \"he would have\", \"he'll\": \"he will\", \"he'll've\": \"he will have\", \"he's\": \"he is\",\n",
        "\"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\", \"i'd\": \"i would\", \"i'd've\": \"i would have\",\n",
        "\"i'll\": \"i will\", \"i'll've\": \"i will have\", \"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
        "\"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\", \"it's\": \"it is\", \"let's\": \"let us\",\n",
        "\"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\", \"mightn't\": \"might not\", \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\",\n",
        "\"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\", \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\",\n",
        "\"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she shall / she will\",\n",
        "\"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\n",
        "\"so've\": \"so have\", \"so's\": \"so is\", \"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
        "\"there'd've\": \"there would have\", \"there's\": \"there is\", \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\",\n",
        "\"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\",\n",
        "\"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\",\n",
        "\"what'll've\": \"what will have\", \"what're\": \"what are\", \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\",\n",
        "\"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\",\n",
        "\"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
        "\"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\", \"y'all'd've\": \"you all would have\",\n",
        "\"y'all're\": \"you all are\", \"y'all've\": \"you all have\", \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
        "\"you're\": \"you are\", \"you've\": \"you have\"\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMR_WCBwcfQy"
      },
      "source": [
        "contractionsWithAnotherInvertedComma = { \n",
        "\"ain’t\": \"am not\", \"aren’t\": \"are not\", \"can’t\": \"cannot\", \"can’t’ve\": \"cannot have\", \"’cause\": \"because\", \"could’ve\": \"could have\", \"couldn’t\": \"could not\",\n",
        "\"couldn’t’ve\": \"could not have\", \"didn’t\": \"did not\", \"doesn’t\": \"does not\", \"don’t\": \"do not\", \"hadn’t\": \"had not\", \"hadn’t’ve\": \"had not have\",\n",
        "\"hasn’t\": \"has not\", \"haven’t\": \"have not\", \"he’d\": \"he had\", \"he’d’ve\": \"he would have\", \"he’ll\": \"he will\", \"he’ll’ve\": \"he will have\", \"he’s\": \"he is\",\n",
        "\"how’d\": \"how did\", \"how’d’y\": \"how do you\", \"how’ll\": \"how will\", \"how’s\": \"how is\", \"i’d\": \"i would\", \"i’d’ve\": \"i would have\",\n",
        "\"i’ll\": \"i will\", \"i’ll’ve\": \"i will have\", \"i’m\": \"i am\", \"i’ve\": \"i have\", \"isn’t\": \"is not\", \"it’d\": \"it would\",\n",
        "\"it’d’ve\": \"it would have\", \"it’ll\": \"it will\", \"it’ll’ve\": \"it will have\", \"it’s\": \"it is\", \"let’s\": \"let us\",\n",
        "\"ma’am\": \"madam\", \"mayn’t\": \"may not\", \"might’ve\": \"might have\", \"mightn’t\": \"might not\", \"mightn’t’ve\": \"might not have\", \"must’ve\": \"must have\", \"mustn’t\": \"must not\",\n",
        "\"mustn’t’ve\": \"must not have\", \"needn’t\": \"need not\", \"needn’t’ve\": \"need not have\", \"o’clock\": \"of the clock\", \"oughtn’t\": \"ought not\", \"oughtn’t’ve\": \"ought not have\",\n",
        "\"shan’t\": \"shall not\", \"shan’t’ve\": \"shall not have\", \"she’d\": \"she would\", \"she’d’ve\": \"she would have\", \"she’ll\": \"she will\",\n",
        "\"she’ll’ve\": \"she will have\", \"she’s\": \"she is\", \"should’ve\": \"should have\", \"shouldn’t\": \"should not\", \"shouldn’t’ve\": \"should not have\",\n",
        "\"so’ve\": \"so have\", \"so’s\": \"so is\", \"that’d\": \"that would\", \"that’d’ve\": \"that would have\", \"that’s\": \"that is\", \"there’d\": \"there would\",\n",
        "\"there’d’ve\": \"there would have\", \"there’s\": \"there is\", \"they’d\": \"they would\", \"they’d’ve\": \"they would have\", \"they’ll\": \"they will\",\n",
        "\"they’ll’ve\": \"they will have\", \"they’re\": \"they are\", \"they’ve\": \"they have\", \"to’ve\": \"to have\", \"wasn’t\": \"was not\", \"we’d\": \"we would\",\n",
        "\"we’d’ve\": \"we would have\", \"we’ll\": \"we will\", \"we’ll’ve\": \"we will have\", \"we’re\": \"we are\", \"we’ve\": \"we have\", \"weren’t\": \"were not\", \"what’ll\": \"what will\",\n",
        "\"what'll've\": \"what will have\", \"what're\": \"what are\", \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\",\n",
        "\"when’ve\": \"when have\", \"where’d\": \"where did\", \"where’s\": \"where is\", \"where’ve\": \"where have\", \"who’ll\": \"who will\", \"who’ll’ve\": \"who will have\",\n",
        "\"who’s\": \"who is\", \"who’ve\": \"who have\", \"why’s\": \"why is\", \"why’ve\": \"why have\", \"will’ve\": \"will have\", \"won’t\": \"will not\", \"won’t’ve\": \"will not have\",\n",
        "\"would’ve\": \"would have\", \"wouldn’t\": \"would not\", \"wouldn’t’ve\": \"would not have\", \"y’all\": \"you all\", \"y’all’d\": \"you all would\", \"y’all’d’ve\": \"you all would have\",\n",
        "\"y’all’re\": \"you all are\", \"y’all’ve\": \"you all have\", \"you’d\": \"you would\", \"you’d’ve\": \"you would have\", \"you’ll\": \"you will\", \"you’ll’ve\": \"you will have\",\n",
        "\"you’re\": \"you are\", \"you’ve\": \"you have\"\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChgIB9W3FeOv"
      },
      "source": [
        "from COVID19_sentinentanalysissocialmedia.preprocessing.preprocess import preprocess\n",
        "pp_class = preprocess(senwave, contractions, contractionsWithAnotherInvertedComma)\n",
        "senwave['Tweet'] = senwave['Tweet'].apply(lambda x : pp_class.preprocess_tweet(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBack14AXFOI"
      },
      "source": [
        "senwave['Tweet'] = senwave['Tweet'].str.lower()\n",
        "senwave.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLPiJRVW2dpf"
      },
      "source": [
        "def check_coverage(vocab, embeddings_index):\n",
        "    a = {}\n",
        "    oov = {}\n",
        "    k = 0\n",
        "    i = 0\n",
        "    for word in tqdm(vocab):\n",
        "        try:\n",
        "            a[word] = embeddings_index[word]\n",
        "            k += vocab[word]\n",
        "        except:\n",
        "            oov[word] = vocab[word]\n",
        "            i += vocab[word]\n",
        "            pass\n",
        "        \n",
        "    print('Found embeddings for {:.2%} of vocab'.format(len(a) / len(vocab)))\n",
        "    print('Found embeddings for  {:.2%} of all text'.format(k / (k + i)))\n",
        "    sorted_x = sorted(oov.items(), key = operator.itemgetter(1))[::-1]\n",
        "\n",
        "    return sorted_x\n",
        "\n",
        "def build_vocab(sentences, verbose = True):\n",
        "    vocab = {}\n",
        "    for sentence in tqdm(sentences, disable = (not verbose)):\n",
        "        for word in sentence:\n",
        "            try:\n",
        "                vocab[word] += 1\n",
        "            except:\n",
        "                vocab[word] = 1\n",
        "    return vocab\n",
        "\n",
        "def get_coefs(word, *arr):\n",
        "    return word, np.asarray(arr, dtype='float32')\n",
        "\n",
        "\n",
        "def load_embeddings(path):\n",
        "    with open(path, encoding = \"utf-8\") as f:\n",
        "        return dict(get_coefs(*line.strip().split(' ')) for line in f)\n",
        "\n",
        "\n",
        "def build_matrix(word_index, path):\n",
        "    embedding_index = load_embeddings(path)\n",
        "    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
        "    for word, i in word_index.items():\n",
        "        try:\n",
        "            embedding_matrix[i] = embedding_index[word]\n",
        "        except KeyError:\n",
        "            pass\n",
        "    return embedding_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOgnFV196rBo"
      },
      "source": [
        "GLOVE_EMBEDDING_FILE = '/content/COVID19_sentinentanalysissocialmedia/glove.840B.300d.txt'\n",
        "glove_embeddings = load_embeddings(GLOVE_EMBEDDING_FILE)\n",
        "print(f'loaded {len(glove_embeddings)} word vectors ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6QLD7Zm69wZ"
      },
      "source": [
        "vocab = build_vocab(list(senwave['Tweet'].apply(lambda x : x.split())))\n",
        "oov = check_coverage(vocab, glove_embeddings)\n",
        "oov[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bu0C_gvdoe7"
      },
      "source": [
        "from COVID19_sentinentanalysissocialmedia.preprocessing import wordReplace\n",
        "senwave['Tweet'] = senwave['Tweet'].apply(lambda x : wordReplace.bruteGen(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahyd9brWZess"
      },
      "source": [
        "# **Building the model followed by training & testing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTDJW3--k2t9"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "sen_train, sen_test = train_test_split(senwave, test_size=0.2, random_state=1024)\n",
        "sen_val, sen_test = train_test_split(sen_test, test_size=0.5, random_state=1024)\n",
        "\n",
        "sen_train.to_csv(\"train.csv\", index = False)\n",
        "sen_test.to_csv(\"test.csv\", index = False)\n",
        "sen_val.to_csv(\"val.csv\", index = False)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeB5I9lyKumT",
        "outputId": "a6e9972c-98f2-4e48-da0a-adc9de180d8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "sen_train.shape, sen_val.shape, sen_test.shape"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((8000, 13), (1000, 13), (1000, 13))"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8d4yP1tvh44D"
      },
      "source": [
        "import spacy\n",
        "spacy_en = spacy.load('en')\n",
        "\n",
        "def tokenizer(tweet):\n",
        "    tweet = re.sub(r'[\\n]', ' ', tweet)\n",
        "    return [tok.text for tok in spacy_en.tokenizer(tweet)]\n",
        "\n",
        "TWEET = torchtext.legacy.data.Field(sequential = True, lower = True, tokenize = tokenizer)\n",
        "LABEL = torchtext.legacy.data.Field(sequential = False, use_vocab = False)\n",
        "\n",
        "dataFields = [(\"ID\", None), (\"Tweet\", TWEET), (\"Optimistic\", LABEL), (\"Thankful\", LABEL),\n",
        "              (\"Empathetic\", LABEL), (\"Pessimistic\", LABEL), (\"Anxious\", LABEL), (\"Sad\", LABEL),\n",
        "              (\"Annoyed\", LABEL), (\"Denial\", LABEL), (\"Official report\", LABEL),\n",
        "              (\"Surprise\", LABEL), (\"Joking\", LABEL)]\n",
        "\n",
        "train_dataset, test_dataset, valid_dataset = torchtext.legacy.data.TabularDataset.splits(\n",
        "    path = '/content/', train = 'train.csv', test = 'test.csv', validation = 'val.csv', format = 'csv', fields = dataFields, skip_header = True\n",
        "    )"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PV4BJv5vnSuW",
        "outputId": "858487e2-f3bb-40ea-f69e-1e3022cd6fdf"
      },
      "source": [
        "print(\"Number of training samples : {}\\n Number of testing samples : {}\\n Number of validation samples : {}\".format(len(train_dataset), len(test_dataset), len(valid_dataset)))"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training samples : 8000\n",
            " Number of testing samples : 1000\n",
            " Number of validation samples : 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7KFFamhpo7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "6824ce72-b74e-4606-c712-274620892baa"
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "TWEET.build_vocab(train_dataset, vectors = 'glove.840B.300d')"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-66-92ac8008bff7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'cuda'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mTWEET\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'glove.840B.300d'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchtext/legacy/data/field.py\u001b[0m in \u001b[0;36mbuild_vocab\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m                             self.eos_token] + kwargs.pop('specials', [])\n\u001b[1;32m    302\u001b[0m             if tok is not None))\n\u001b[0;32m--> 303\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspecials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspecials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnumericalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchtext/legacy/vocab.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, counter, max_size, min_freq, specials, vectors, unk_init, vectors_cache, specials_first)\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvectors\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munk_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munk_init\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvectors_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0munk_init\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mvectors_cache\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchtext/legacy/vocab.py\u001b[0m in \u001b[0;36mload_vectors\u001b[0;34m(self, vectors, **kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m                         \"vectors are {}\".format(\n\u001b[1;32m    177\u001b[0m                             vector, list(pretrained_aliases.keys())))\n\u001b[0;32m--> 178\u001b[0;31m                 \u001b[0mvectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpretrained_aliases\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m                 raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchtext/vocab.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, dim, **kwargs)\u001b[0m\n\u001b[1;32m    483\u001b[0m         \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'glove.{}.{}d.txt'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 485\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGloVe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchtext/vocab.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, cache, url, unk_init, max_vectors)\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munk_init\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0munk_init\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0munk_init\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_vectors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchtext/vocab.py\u001b[0m in \u001b[0;36mcache\u001b[0;34m(self, name, cache, url, max_vectors)\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loading vectors from {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_pt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_pt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    605\u001b[0m                     \u001b[0mopened_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_position\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m    880\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnpicklerWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_loaded_sparse_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m    855\u001b[0m         \u001b[0mdata_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 857\u001b[0;31m             \u001b[0mload_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_maybe_decode_ascii\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    858\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload_tensor\u001b[0;34m(data_type, size, key, location)\u001b[0m\n\u001b[1;32m    843\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 845\u001b[0;31m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_storage_from_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    846\u001b[0m         \u001b[0mloaded_storages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrestore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C24Vz979koIL"
      },
      "source": [
        "#**Using BERT**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "LTclW9Vdkspz",
        "outputId": "70349b73-ce8c-4e0c-c8d2-ca7ac8e5382b"
      },
      "source": [
        "df = senwave.drop(['ID'], axis = 1)\n",
        "df['list'] = df[df.columns[1:12]].values.tolist()\n",
        "new_df = df[['Tweet', 'list']].copy()\n",
        "new_df.head()"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Tweet</th>\n",
              "      <th>list</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>a glass of wine keeps the corona away  drake  ...</td>\n",
              "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>can anyone tell me if you took the flu shot la...</td>\n",
              "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>by the way producers send me beats im working ...</td>\n",
              "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>when someone you know   apart of your family d...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>dear soccer  i really miss you  please come ba...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               Tweet                               list\n",
              "0  a glass of wine keeps the corona away  drake  ...  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
              "1  can anyone tell me if you took the flu shot la...  [0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0]\n",
              "2  by the way producers send me beats im working ...  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
              "3  when someone you know   apart of your family d...  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
              "4  dear soccer  i really miss you  please come ba...  [0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQnM-yH2m4PN"
      },
      "source": [
        "**Preparing the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdVPPBlzm3Wy"
      },
      "source": [
        "MAX_LEN = 200 #based on length of tweets\n",
        "TRAIN_BATCH_SIZE = 1\n",
        "VALID_BATCH_SIZE = 1\n",
        "TEST_BATCH_SIZE = 1\n",
        "EPOCHS = 4\n",
        "LEARNING_RATE = 2e-5 #tried 1e-03, 1e-04, 1e-05\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBA1bIkAnczz"
      },
      "source": [
        "class CustomDataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.dataframe = dataframe\n",
        "        self.tweet = dataframe['Tweet']\n",
        "        self.targets = self.dataframe.list\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tweet)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        tweet = str(self.tweet[index])\n",
        "        tweet = \" \".join(tweet.split())\n",
        "        target = self.targets[index]\n",
        "        \n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            tweet,\n",
        "            None,\n",
        "            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "            max_length = self.max_len,\n",
        "            return_token_type_ids = True,\n",
        "            pad_to_max_length = True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',  # Return PyTorch tensors\n",
        "        )\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "        token_type_ids = inputs['token_type_ids']\n",
        "\n",
        "        return {\n",
        "            'tweet': tweet,\n",
        "            'ids' : torch.tensor(ids, dtype = torch.long),\n",
        "            'mask' : torch.tensor(mask, dtype = torch.long),\n",
        "            'targets' : torch.tensor(target, dtype = torch.float)\n",
        "        }"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIvrUTMhpjYl"
      },
      "source": [
        "train_dataset = sen_train.drop(['ID'], axis = 1)\n",
        "train_dataset['list'] = train_dataset[train_dataset.columns[1:12]].values.tolist()\n",
        "train_df = train_dataset[['Tweet', 'list']].copy()\n",
        "train_df = train_df.reset_index(drop = True)\n",
        "\n",
        "test_dataset = sen_test.drop(['ID'], axis = 1)\n",
        "test_dataset['list'] = test_dataset[test_dataset.columns[1:12]].values.tolist()\n",
        "test_df = test_dataset[['Tweet', 'list']].copy()\n",
        "test_df = test_df.reset_index(drop = True)\n",
        "\n",
        "valid_dataset = sen_val.drop(['ID'], axis = 1)\n",
        "valid_dataset['list'] = valid_dataset[valid_dataset.columns[1:12]].values.tolist()\n",
        "valid_df = valid_dataset[['Tweet', 'list']].copy()\n",
        "valid_df = valid_df.reset_index(drop = True)"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiH5FmKFPdQR"
      },
      "source": [
        "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
        "  ds = CustomDataset(\n",
        "      tokenizer = tokenizer,\n",
        "      dataframe = df,\n",
        "      max_len=max_len,\n",
        "\n",
        "  )\n",
        "\n",
        "  return DataLoader(\n",
        "    ds,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=0\n",
        "  )"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6MkfDn6qi0l"
      },
      "source": [
        "training_loader = create_data_loader(train_df, tokenizer, MAX_LEN, TRAIN_BATCH_SIZE)\n",
        "validation_loader = create_data_loader(valid_df, tokenizer, MAX_LEN, VALID_BATCH_SIZE)\n",
        "testing_loader = create_data_loader(test_df, tokenizer, MAX_LEN, TEST_BATCH_SIZE)"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGyUsxh5a9pk",
        "outputId": "317150cf-c384-4cb5-a022-581c2b7fce7f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "data = next(iter(training_loader))\n",
        "data.keys()\n",
        "\n",
        "print(data['ids'].squeeze(0).shape)\n",
        "print(data['mask'].shape)\n",
        "print(data['targets'].squeeze(0).shape)"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 200])\n",
            "torch.Size([1, 1, 200])\n",
            "torch.Size([11])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_S9ubAVrXUn"
      },
      "source": [
        "**Creating the Transformer model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaM3c69ZrWLf"
      },
      "source": [
        "class BERT(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layer1 = transformers.BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.layer2 = nn.Dropout(0.3)\n",
        "        self.layer3 = nn.Linear(768, 11)\n",
        "\n",
        "    def forward(self, ids, mask):\n",
        "        unw, out_1 = self.layer1(ids, attention_mask = mask)[0], self.layer1(ids, attention_mask = mask)[1]\n",
        "        out_2 = self.layer2(out_1)\n",
        "        out_final = self.layer3(out_2)\n",
        "        return out_final\n",
        "\n",
        "model = BERT()\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpKk20Oiun51"
      },
      "source": [
        "def loss_fn(outputs, targets):\n",
        "    return nn.BCEWithLogitsLoss()(outputs, targets)\n",
        "\n",
        "optimizer = AdamW(params = model.parameters(), lr = LEARNING_RATE)\n",
        "total_steps = len(training_loader) * EPOCHS\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "  optimizer,\n",
        "  num_warmup_steps=0,\n",
        "  num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "loss_fn = nn.BCEWithLogitsLoss().to(device)"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRTGpC_FvL78"
      },
      "source": [
        "**Fine Tuning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bk8aWkYqT6L3"
      },
      "source": [
        "def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n",
        "  model = model.train()\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "  \n",
        "  for d in data_loader:\n",
        "    ids = d[\"ids\"].squeeze(0).to(device)\n",
        "    mask = d[\"mask\"].squeeze(0).to(device)\n",
        "    targets = d[\"targets\"].to(device)\n",
        "\n",
        "    outputs = model(\n",
        "      ids=ids,\n",
        "      mask=mask\n",
        "    )\n",
        "\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    loss = loss_fn(outputs, targets)\n",
        "\n",
        "    correct_predictions += torch.sum(preds == targets)\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rx1-IsLEUBSh"
      },
      "source": [
        "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
        "  model = model.eval()\n",
        "\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "      ids = d[\"ids\"].squeeze(0).to(device)\n",
        "      mask = d[\"mask\"].squeeze(0).to(device)\n",
        "      targets = d[\"targets\"].to(device)\n",
        "\n",
        "      outputs = model(\n",
        "        ids=ids,\n",
        "        mask=mask\n",
        "      )\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "      loss = loss_fn(outputs, targets)\n",
        "\n",
        "      correct_predictions += torch.sum(preds == targets)\n",
        "      losses.append(loss.item())\n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AK9DaVr0UQHz",
        "outputId": "e9d64dd9-42e5-44b4-fc77-8ef3811f8976",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "EPOCHS = 4\n",
        "\n",
        "\n",
        "history = defaultdict(list)\n",
        "best_accuracy = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "  print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
        "  print('-' * 10)\n",
        "\n",
        "  train_acc, train_loss = train_epoch(\n",
        "    model,\n",
        "    training_loader,    \n",
        "    loss_fn, \n",
        "    optimizer, \n",
        "    device, \n",
        "    scheduler, \n",
        "    len(train_df)\n",
        "  )\n",
        "\n",
        "  print(f'Train loss {train_loss} accuracy {train_acc}')\n",
        "\n",
        "  val_acc, val_loss = eval_model(\n",
        "    model,\n",
        "    validation_loader,\n",
        "    loss_fn, \n",
        "    device, \n",
        "    len(valid_df)\n",
        "  )\n",
        "\n",
        "  print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
        "  print()\n",
        "\n",
        "  history['train_acc'].append(train_acc)\n",
        "  history['train_loss'].append(train_loss)\n",
        "  history['val_acc'].append(val_acc)\n",
        "  history['val_loss'].append(val_loss)\n",
        "\n",
        "  if val_acc > best_accuracy:\n",
        "    torch.save(model, '/content/COVID19_sentinentanalysissocialmedia/models/bertmodel-new.pth')\n",
        "    best_accuracy = val_acc\n",
        "  print(best_accuracy)"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4\n",
            "----------\n",
            "Train loss 0.36887306316103785 accuracy 1.0005\n",
            "Val   loss 0.33389336156472565 accuracy 0.89\n",
            "\n",
            "tensor(0.8900, device='cuda:0', dtype=torch.float64)\n",
            "Epoch 2/4\n",
            "----------\n",
            "Train loss 0.28592678264610005 accuracy 1.11775\n",
            "Val   loss 0.36135224744305017 accuracy 1.226\n",
            "\n",
            "tensor(1.2260, device='cuda:0', dtype=torch.float64)\n",
            "Epoch 3/4\n",
            "----------\n",
            "Train loss 0.2147768516429933 accuracy 1.1195\n",
            "Val   loss 0.4114081586119719 accuracy 1.348\n",
            "\n",
            "tensor(1.3480, device='cuda:0', dtype=torch.float64)\n",
            "Epoch 4/4\n",
            "----------\n",
            "Train loss 0.15852845168864588 accuracy 1.1027500000000001\n",
            "Val   loss 0.4343771455541719 accuracy 1.199\n",
            "\n",
            "tensor(1.3480, device='cuda:0', dtype=torch.float64)\n",
            "CPU times: user 2h 4min 20s, sys: 2min, total: 2h 6min 20s\n",
            "Wall time: 2h 6min 16s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hvyaf9FVry1",
        "outputId": "c41fc244-ed3b-48b5-a321-ce0241f5e55e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "source": [
        "plt.plot(history['train_acc'], label='train accuracy')\n",
        "plt.plot(history['val_acc'], label='validation accuracy')\n",
        "\n",
        "plt.title('Training history')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.ylim([0, 1]);"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-0d8ecf63c1ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'validation accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training history'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6e0hWO8tu2mp"
      },
      "source": [
        "# def train(epoch):\n",
        "#     model.train()\n",
        "#     total_loss = 0\n",
        "\n",
        "#     for unw, data in enumerate(training_loader, 0):\n",
        "#         ids = data['ids'].to(device, dtype = torch.long)\n",
        "#         mask = data['mask'].to(device, dtype = torch.long)\n",
        "#         token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "#         targets = data['targets'].to(device, dtype = torch.float)\n",
        "\n",
        "#         outputs = model(ids, mask, token_type_ids, return_dict = False)\n",
        "#         loss = loss_fn(outputs, targets)\n",
        "#         total_loss += loss.item()\n",
        "\n",
        "#         if unw % 2000 == 0:\n",
        "#             print(f'Iter : {unw+1}, Epoch: {epoch+1}, Loss: {total_loss/(unw+1)}')\n",
        "\n",
        "#         optimizer.zero_grad()\n",
        "#         loss.backward()\n",
        "#         optimizer.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HEDCGLYv8GO"
      },
      "source": [
        "# EPOCHS = 4\n",
        "# for epoch in range(EPOCHS):\n",
        "#     train(epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ytd-pK6F3lz"
      },
      "source": [
        "**Validating the data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFk6clJLF5uC"
      },
      "source": [
        "# def valid():\n",
        "#     model.eval()\n",
        "#     req_targets = []\n",
        "#     req_outputs = []\n",
        "#     valid_loss = 0\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         for unw, data in enumerate(testing_loader, 0):\n",
        "#             ids = data['ids'].to(device, dtype = torch.long)\n",
        "#             mask = data['mask'].to(device, dtype = torch.long)\n",
        "#             token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "#             targets = data['targets'].to(device, dtype = torch.float)\n",
        "#             outputs = model(ids, mask, token_type_ids)\n",
        "#             loss = loss_fn(outputs, targets)\n",
        "#             valid_loss += loss.item()\n",
        "\n",
        "#             req_targets.extend(targets.cpu().detach().numpy().tolist())\n",
        "#             req_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
        "#     valid_loss /= len(testing_loader)\n",
        "#     return req_outputs, req_targets, valid_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZUEwg2dGpM-"
      },
      "source": [
        "# from sklearn import metrics\n",
        "\n",
        "# outputs, targets, valid_loss = valid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehXGXlKwnhG5"
      },
      "source": [
        "# outputs = np.array(outputs)\n",
        "# targets = np.array(targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJL0Xvikn6Bk"
      },
      "source": [
        "# int_outputs = np.zeros((outputs.shape[0], outputs.shape[1]))\n",
        "\n",
        "# for row in range(outputs.shape[0]):\n",
        "#     for col in range(outputs.shape[1]):\n",
        "#         if outputs[row][col] >= 0.5: int_outputs[row][col] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6Q8XTluoZUR"
      },
      "source": [
        "# targets[0], int_outputs[0], outputs[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1jBAiNrpIgi"
      },
      "source": [
        "# bert_ham_loss = hamming_loss(targets, int_outputs)\n",
        "# bert_jacc_score = jaccard_score(targets, int_outputs, average = 'samples')\n",
        "# bert_lrap = label_ranking_average_precision_score(targets, outputs)\n",
        "# bert_f1_macro = f1_score(targets, int_outputs, average = 'macro')\n",
        "# bert_f1_micro = f1_score(targets, int_outputs, average = 'micro')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3b1YLJmZqTjX"
      },
      "source": [
        "# print(\"Test Loss:\", valid_loss)\n",
        "# print(\"Hamming Loss:\", bert_ham_loss)\n",
        "# print(\"Jaccard Score:\", bert_jacc_score)\n",
        "# print(\"Label Ranking Average Precision Score:\", bert_lrap)\n",
        "# print(\"F1 Macro Score:\", bert_f1_macro)\n",
        "# print(\"F1 Micro Score:\", bert_f1_micro)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uybVXteItKmD"
      },
      "source": [
        "Results:\n",
        "\n",
        "\n",
        "1.   lr = 1e-03: bce loss: 0.432, hamming loss: 0.183, jacc score: 0, lrap: 0.5659, f1 macro: 0, f1 micro: 0\n",
        "2.   lr = 1e-04: bce loss: 0.432, hamming loss: 0.183, jacc score: 0, lrap: 0.5275, f1 macro: 0, f1 micro: 0\n",
        "3.   lr = 1e-05: bce loss: 0.372, hamming loss: 0.142, jacc score: 0.5096, lrap: 0.766, f1 macro: 0.53, f1 micro: 0.587 \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Neye1aWVrETw"
      },
      "source": [
        "# torch.save(model, f = '/content/COVID19_sentinentanalysissocialmedia/models/bertmodel.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buDnFn21kxoo"
      },
      "source": [
        "# **Using LSTM**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyyvY65EBJI-"
      },
      "source": [
        "**Hyperparameters to be considered**\n",
        "*  Learning Rate\n",
        "*  Hidden Dimension of LSTM\n",
        "*  Dropout Probability\n",
        "*  Threshold\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLuNLQg02P0F"
      },
      "source": [
        "vocab = TWEET.vocab\n",
        "BATCH_SIZE = 32\n",
        "n_label = 11\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iter, test_iter = data.BucketIterator.splits(datasets = (train_dataset, test_dataset),\n",
        "                                                   batch_size = BATCH_SIZE,\n",
        "                                                   sort_key = lambda x : len(x.Tweet),\n",
        "                                                   sort_within_batch = False,\n",
        "                                                   repeat = False,\n",
        "                                                   device = device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsuVaSid0bAb"
      },
      "source": [
        "np.random.seed(1024)\n",
        "for i in range(TWEET.vocab.vectors.shape[0]):\n",
        "    vec = TWEET.vocab.vectors[i]\n",
        "    if torch.sum(vec).item() == 0:\n",
        "        a = np.random.uniform(-0.25, 0.25, 300)\n",
        "        TWEET.vocab.vectors[i] = torch.from_numpy(a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGziX-x6cSRU"
      },
      "source": [
        "batch = next(train_iter.__iter__()); batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stOmTXDreL0Z"
      },
      "source": [
        "#idea taken from http://mlexplained.com/2018/02/08/a-comprehensive-tutorial-to-torchtext/\n",
        "class BatchWrapper():\n",
        "    def __init__(self, dl, x_var, y_vars):\n",
        "        self.dl = dl\n",
        "        self.x_var = x_var\n",
        "        self.y_vars = y_vars\n",
        "\n",
        "    def __iter__(self):\n",
        "        for batch in self.dl:\n",
        "            x = getattr(batch, self.x_var)\n",
        "            if self.y_vars is not None:\n",
        "                y = torch.cat([getattr(batch, feat).unsqueeze(1) for feat in self.y_vars], dim = 1).float()\n",
        "            else:\n",
        "                y = torch.zeros((1))\n",
        "            yield(x, y)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.dl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zg5H4OCogXtH"
      },
      "source": [
        "train_dl = BatchWrapper(train_iter, \"Tweet\", ['Optimistic', 'Thankful', 'Empathetic', 'Pessimistic', \n",
        "                                              'Anxious', 'Sad', 'Annoyed', 'Denial', 'Official report', 'Surprise', 'Joking'])\n",
        "test_dl = BatchWrapper(test_iter, \"Tweet\", ['Optimistic', 'Thankful', 'Empathetic', 'Pessimistic', \n",
        "                                            'Anxious', 'Sad', 'Annoyed', 'Denial', 'Official report', 'Surprise', 'Joking'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48UVabxn_nSE"
      },
      "source": [
        "class CustomLSTM(nn.Module):\n",
        "    def __init__(self, embedding_dim, vocab, hidden_dim, output_dim, drop_prob, bidirectional = False, use_glove = True):\n",
        "        super().__init__()\n",
        "        self.embeddings = nn.Embedding(len(vocab), embedding_dim)\n",
        "        if use_glove:\n",
        "            self.embeddings.weight.data.copy_(vocab.vectors)\n",
        "            self.embeddings.weight.requires_grad = False\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional = bidirectional, batch_first = True, num_layers = 2)\n",
        "        if bidirectional is True:\n",
        "            self.lin = nn.Linear(2*hidden_dim, 64)\n",
        "        else:\n",
        "            self.lin = nn.Linear(hidden_dim, 64)\n",
        "        self.fc = nn.Linear(64, output_dim)\n",
        "        self.dropout = nn.Dropout(p = drop_prob)\n",
        "    \n",
        "    def forward(self, sentence):\n",
        "        #sentence = [max_len, batch_size]\n",
        "\n",
        "        embed = self.embeddings(torch.transpose(sentence, 0, 1))\n",
        "        #embed = [batch_size, max_len, embedding_dim]\n",
        "        \n",
        "        if self.drop_prob:\n",
        "            embed = self.dropout(embed)\n",
        "        \n",
        "        lstm_out, (hidden, cell) = self.lstm(embed)\n",
        "        #lstm_out = [batch_size, max_len, 2*hidden_dim if bidirectional else hidden_dim]\n",
        "        #hidden = [num_layers, batch_size, hidden_dim]\n",
        "        #cell = [num_layers, batch_size, hidden_dim]\n",
        "        \n",
        "        out = lstm_out[:,-1,:].squeeze()\n",
        "        #out = [batch_size, 2*hidden_dim if bidirectional else hidden_dim]\n",
        "        \n",
        "        out = self.lin(out)\n",
        "        #out = [batch_size, 64]\n",
        "\n",
        "        outputs = self.fc(out)\n",
        "        #outputs = [batch_size, output_dim]\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ifJTTfCIKme"
      },
      "source": [
        "def evaluation_metrics(actual_labels, pred_labels, threshold):\n",
        "    int_pred_labels = pred_labels\n",
        "    for i in range(len(pred_labels)):\n",
        "        for j in range(11):\n",
        "            if int_pred_labels[i][j] >= threshold: int_pred_labels[i][j] = 1\n",
        "            else:\n",
        "                int_pred_labels[i][j] = 0\n",
        "    \n",
        "    ham_loss = hamming_loss(actual_labels, int_pred_labels)\n",
        "    jacc_score = jaccard_score(actual_labels, int_pred_labels, average = 'samples')\n",
        "    lrap = label_ranking_average_precision_score(actual_labels, pred_labels)\n",
        "    f1_macro = f1_score(actual_labels, int_pred_labels, average = 'macro')\n",
        "    f1_micro = f1_score(actual_labels, int_pred_labels, average = 'micro')\n",
        "\n",
        "    return ham_loss, jacc_score, lrap, f1_macro, f1_micro"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgZ3c5GvPCZb"
      },
      "source": [
        "def train(model, loss_fn, optimizer, n_epochs, train_dl, threshold):\n",
        "\n",
        "    train_losses = []  \n",
        "    hamming_losses = []\n",
        "    jaccard_scores = []\n",
        "    lraps = []  \n",
        "    iter = 1\n",
        "\n",
        "    for epoch in range(1, n_epochs+1):\n",
        "        running_loss = 0.0\n",
        "        pred_labels = []\n",
        "        actual_labels = []\n",
        "        model.train()\n",
        "        for x, y in train_dl:\n",
        "            #print(x.shape, y.shape)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            preds = model(x)\n",
        "\n",
        "            m = nn.Sigmoid()\n",
        "            sig_preds = m(preds)\n",
        "            \n",
        "            for tens in sig_preds:\n",
        "                pred_labels.append(tens.cpu().detach().numpy())\n",
        "            for tens in y:\n",
        "                actual_labels.append(tens.cpu().detach().numpy())\n",
        "\n",
        "            loss = loss_fn(preds, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * x.shape[0]\n",
        "\n",
        "        ham_loss, jacc_score, lrap, f1_macro, f1_micro = evaluation_metrics(actual_labels, pred_labels, threshold)\n",
        "\n",
        "        epoch_loss = running_loss / len(train_dataset)\n",
        "        train_losses.append(epoch_loss)\n",
        "        hamming_losses.append(ham_loss)\n",
        "        lraps.append(lrap)\n",
        "        jaccard_scores.append(jacc_score)\n",
        "        '''\n",
        "        if iter % 5 == 0:\n",
        "            print(\"Epoch: \", epoch)\n",
        "            print(\"Binary Cross Entropy With Logits Loss: {:.4f}\".format(epoch_loss))\n",
        "            print(\"Hamming Loss : {:.4f}\".format(ham_loss))\n",
        "            print(\"Jaccard Score: {:.4f}\".format(jacc_score))\n",
        "            print(\"Label Ranking Average Precision Score: {:.4f}\".format(lrap))\n",
        "            print(\"F1 Macro Score: {:.4f}\".format(f1_macro))\n",
        "            print(\"F1 Micro Score: {:.4f}\".format(f1_micro))\n",
        "            print(\"\\n\")\n",
        "        iter += 1\n",
        "        '''\n",
        "    return train_losses, hamming_losses, jaccard_scores, lraps, f1_macro, f1_micro"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJo2hk-oWGVa"
      },
      "source": [
        "def test(model, loss_fn, test_dl, threshold):\n",
        "    running_loss = 0.0\n",
        "    pred_labels = []\n",
        "    actual_labels = []\n",
        "    model.eval()\n",
        "    for x, y in test_dl:\n",
        "        #print(x.shape, y.shape)\n",
        "\n",
        "        preds = model(x)\n",
        "\n",
        "        m = nn.Sigmoid()\n",
        "        sig_preds = m(preds)\n",
        "        \n",
        "        for tens in sig_preds:\n",
        "            pred_labels.append(tens.cpu().detach().numpy())\n",
        "        for tens in y:\n",
        "            actual_labels.append(tens.cpu().detach().numpy())\n",
        "\n",
        "        loss = loss_fn(preds, y)\n",
        "\n",
        "        running_loss += loss.item() * x.shape[0]\n",
        "\n",
        "    ham_loss, jacc_score, lrap, f1_macro, f1_micro = evaluation_metrics(actual_labels, pred_labels, threshold)\n",
        "\n",
        "    test_loss = running_loss / len(test_dataset)\n",
        "    return test_loss, ham_loss, jacc_score, lrap, f1_macro, f1_micro"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lhSR_xR63mh"
      },
      "source": [
        "# **Experimentation with various combinations of the hyperparameters**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yq3DZBRekaRL"
      },
      "source": [
        "## Till 22-12-2020"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hw5DLkDMI4Fx"
      },
      "source": [
        "learning_rates = [1e-3]\n",
        "hidden_dims = [128, 256]\n",
        "thresholds = [0.4, 0.45, 0.5]\n",
        "dropouts = [True, False]\n",
        "\n",
        "all_models = []\n",
        "iter = 1\n",
        "#number of iterations = 3*2*2 = 12 = number of models\n",
        "\n",
        "for learning_rate in learning_rates:\n",
        "    for hidden_dim in hidden_dims:\n",
        "        for threshold in thresholds:\n",
        "            for dropout in dropouts:\n",
        "                model = CustomLSTM(embedding_dim = vocab.vectors.shape[1], vocab = vocab, hidden_dim = hidden_dim, output_dim = 11, drop_prob = dropout)\n",
        "                model = model.to(device)\n",
        "\n",
        "                optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
        "                loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "                train_loss, hamm_loss, jacc_score, lrap, f1_macro, f1_micro = train(model, loss_fn, optimizer, 50, train_dl, threshold)\n",
        "                all_models.append(model)\n",
        "                print(\"Iteration : {}\".format(iter))\n",
        "                print(\"Learning Rate : {}, Hidden Dimension : {}, Threshold : {}, Dropout = {}\".format(learning_rate, hidden_dim, threshold, dropout))\n",
        "                print(\"Binary Cross Entropy With Logits Loss: {:.4f}\".format(min(train_loss)))\n",
        "                print(\"Hamming Loss : {:.4f}\".format(min(hamm_loss)))\n",
        "                print(\"Jaccard Score: {:.4f}\".format(max(jacc_score)))\n",
        "                print(\"Label Ranking Average Precision Score: {:.4f}\".format(max(lrap)))\n",
        "                print(\"F1 Macro Score: {:.4f}\".format(f1_macro))\n",
        "                print(\"F1 Micro Score: {:.4f}\".format(f1_micro))\n",
        "                print(\"\\n\")\n",
        "\n",
        "                test_loss, test_hamm_loss, test_jacc_score, test_lrap, test_f1_macro, test_f1_micro = test(model, loss_fn, test_dl, threshold)\n",
        "                print(\"TestBinary Cross Entropy With Logits Loss: {:.4f}\".format(test_loss))\n",
        "                print(\"Test Hamming Loss : {:.4f}\".format(test_hamm_loss))\n",
        "                print(\"Test Jaccard Score: {:.4f}\".format(test_jacc_score))\n",
        "                print(\"Test Label Ranking Average Precision Score: {:.4f}\".format(test_lrap))\n",
        "                print(\"Test F1 Macro Score: {:.4f}\".format(test_f1_macro))\n",
        "                print(\"Test F1 Micro Score: {:.4f}\".format(test_f1_micro))\n",
        "                print(\"\\n\")\n",
        "                print(\"----------------------------------------------------------------\")\n",
        "                iter += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arCjOqoikmhG"
      },
      "source": [
        "## 23-12-2020\n",
        "*  taking inference from the previous days, having a dropout in the LSTM layer has always lowered the overfitting. Hence we will be using dropout layers in all models today.\n",
        "*  We shall experiment with the values of the dropout probability today"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9jAW-ZjkpP_"
      },
      "source": [
        "learning_rates = [1e-3]\n",
        "hidden_dims = [128, 256]\n",
        "thresholds = [0.4, 0.45, 0.5]\n",
        "drop_probs = [0.4, 0.5, 0.6, 0.7]\n",
        "\n",
        "all_models = []\n",
        "iter = 1\n",
        "#number of iterations = 4*3*2 = 24 = number of models\n",
        "\n",
        "for learning_rate in learning_rates:\n",
        "    for hidden_dim in hidden_dims:\n",
        "        for threshold in thresholds:\n",
        "            for drop_prob in drop_probs:\n",
        "                model = CustomLSTM(embedding_dim = vocab.vectors.shape[1], vocab = vocab, hidden_dim = hidden_dim, output_dim = 11, drop_prob = drop_prob)\n",
        "                model = model.to(device)\n",
        "\n",
        "                optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
        "                loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "                train_loss, hamm_loss, jacc_score, lrap, f1_macro, f1_micro = train(model, loss_fn, optimizer, 50, train_dl, threshold)\n",
        "                all_models.append(model)\n",
        "                print(\"Iteration : {}\".format(iter))\n",
        "                print(\"Learning Rate : {}, Hidden Dimension : {}, Threshold : {}, Dropout Probability = {}\".format(learning_rate, hidden_dim, threshold, drop_prob))\n",
        "                print(\"Binary Cross Entropy With Logits Loss: {:.4f}\".format(min(train_loss)))\n",
        "                print(\"Hamming Loss : {:.4f}\".format(min(hamm_loss)))\n",
        "                print(\"Jaccard Score: {:.4f}\".format(max(jacc_score)))\n",
        "                print(\"Label Ranking Average Precision Score: {:.4f}\".format(max(lrap)))\n",
        "                print(\"F1 Macro Score: {:.4f}\".format(f1_macro))\n",
        "                print(\"F1 Micro Score: {:.4f}\".format(f1_micro))\n",
        "                print(\"\\n\")\n",
        "\n",
        "                test_loss, test_hamm_loss, test_jacc_score, test_lrap, test_f1_macro, test_f1_micro = test(model, loss_fn, test_dl, threshold)\n",
        "                print(\"TestBinary Cross Entropy With Logits Loss: {:.4f}\".format(test_loss))\n",
        "                print(\"Test Hamming Loss : {:.4f}\".format(test_hamm_loss))\n",
        "                print(\"Test Jaccard Score: {:.4f}\".format(test_jacc_score))\n",
        "                print(\"Test Label Ranking Average Precision Score: {:.4f}\".format(test_lrap))\n",
        "                print(\"Test F1 Macro Score: {:.4f}\".format(test_f1_macro))\n",
        "                print(\"Test F1 Micro Score: {:.4f}\".format(test_f1_micro))\n",
        "                print(\"\\n\")\n",
        "                print(\"----------------------------------------------------------------\")\n",
        "                iter += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UW8_mcxG3rN2"
      },
      "source": [
        "# **Some inferences**\n",
        "*  Best Test BCE with logits loss at Iteration 12<br>\n",
        "*  Best Test Hamming Loss at Iteration 12<br>\n",
        "*  Best Test Jaccard Score at Iteration 3 & 16<br>\n",
        "*  Best Test LRAP Score at Iteration 11<br>\n",
        "*  Best Test F1 Macro Score at Iteration 16<br>\n",
        "*  Best Test F1 Micro Score at Iteration 3<br>\n",
        "<br> <br>\n",
        "*  This indicates that we need only consider hyperparams corresponding to iterations 3, 11, 12 and 16\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64a_b52n-Ho6"
      },
      "source": [
        " **Hyperparameters to be considered for the final steps**:\n",
        "\n",
        "*   Learning Rate: 0.001\n",
        "*   Hidden Dimension: 3, 11, 12 = 128 & 16 = 256\n",
        "*   Threshold: 3 & 16 = 0.4, 11 & 12 = 0.5\n",
        "*   Dropout Probability: 3 & 11 = 0.6, 12 & 16 = 0.7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUwjKhxd8qyi"
      },
      "source": [
        "Since we see that either the best hyperparameters correspond to dropout probability = 0.6 or 0.7, we use dropout probability = 0.65 which is verified below as performing nearly the same or better"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSBb2r1YJqrq"
      },
      "source": [
        "## Final evaluation using narrowed down hyperparameters\n",
        "26-12-2020"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y46-8KJdqHvl"
      },
      "source": [
        "%%time\n",
        "learning_rate = 1e-3\n",
        "hidden_dim = 128\n",
        "threshold = 0.5\n",
        "drop_prob = 0.65\n",
        "\n",
        "all_bce_losses = []\n",
        "all_hamm_losses = []\n",
        "all_jacc_scores = []\n",
        "all_lraps = []\n",
        "all_f1_macro = []\n",
        "all_f1_micro = []\n",
        "all_models = []\n",
        "\n",
        "for exp in range(1, 11):\n",
        "\n",
        "    print(\"Experiment {}\".format(exp), '\\n\\n')\n",
        "\n",
        "    model = CustomLSTM(embedding_dim = vocab.vectors.shape[1], vocab = vocab, hidden_dim = hidden_dim, output_dim = 11, drop_prob = drop_prob)\n",
        "    model = model.to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
        "    loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    train_loss, hamm_loss, jacc_score, lrap, f1_macro, f1_micro = train(model, loss_fn, optimizer, 50, train_dl, threshold)\n",
        "    torch.save(model, 'model{}.pth'.format(exp))\n",
        "    '''\n",
        "    print(\"Learning Rate : {}, Hidden Dimension : {}, Threshold : {}, Dropout Probability = {}\".format(learning_rate, hidden_dim, threshold, drop_prob))\n",
        "    print(\"Binary Cross Entropy With Logits Loss: {:.4f}\".format(min(train_loss)))\n",
        "    print(\"Hamming Loss : {:.4f}\".format(min(hamm_loss)))\n",
        "    print(\"Jaccard Score: {:.4f}\".format(max(jacc_score)))\n",
        "    print(\"Label Ranking Average Precision Score: {:.4f}\".format(max(lrap)))\n",
        "    print(\"F1 Macro Score: {:.4f}\".format(f1_macro))\n",
        "    print(\"F1 Micro Score: {:.4f}\".format(f1_micro))\n",
        "    print(\"\\n\")\n",
        "    '''\n",
        "\n",
        "    test_loss, test_hamm_loss, test_jacc_score, test_lrap, test_f1_macro, test_f1_micro = test(model, loss_fn, test_dl, threshold)\n",
        "    all_bce_losses.append(test_loss)\n",
        "    all_hamm_losses.append(test_hamm_loss)\n",
        "    all_jacc_scores.append(test_jacc_score)\n",
        "    all_lraps.append(test_lrap)\n",
        "    all_f1_macro.append(test_f1_macro)\n",
        "    all_f1_micro.append(test_f1_micro)\n",
        "\n",
        "\n",
        "    print(\"TestBinary Cross Entropy With Logits Loss: {:.4f}\".format(test_loss))\n",
        "    print(\"Test Hamming Loss : {:.4f}\".format(test_hamm_loss))\n",
        "    print(\"Test Jaccard Score: {:.4f}\".format(test_jacc_score))\n",
        "    print(\"Test Label Ranking Average Precision Score: {:.4f}\".format(test_lrap))\n",
        "    print(\"Test F1 Macro Score: {:.4f}\".format(test_f1_macro))\n",
        "    print(\"Test F1 Micro Score: {:.4f}\".format(test_f1_micro))\n",
        "    print(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NB4ms2mOp8Ee"
      },
      "source": [
        "print(\"Average Binary Cross Entropy With Logits Loss: {:.4f}\".format(sum(all_bce_losses)/10))\n",
        "print(\"Average Hamming Loss : {:.4f}\".format(sum(all_hamm_losses)/10))\n",
        "print(\"Average Jaccard Score: {:.4f}\".format(sum(all_jacc_scores)/10))\n",
        "print(\"Average Label Ranking Average Precision Score: {:.4f}\".format(sum(all_lraps)/10))\n",
        "print(\"Average F1 Macro Score: {:.4f}\".format(sum(all_f1_macro)/10))\n",
        "print(\"Average F1 Micro Score: {:.4f}\".format(sum(all_f1_micro)/10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_fA8EPqvzsB"
      },
      "source": [
        "**Model 7 performed the best, so I downloaded the weights**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWZ735YQBrzC"
      },
      "source": [
        "## Using a different value of dropout probability"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAZtbnqJB4aX"
      },
      "source": [
        "%%time\n",
        "learning_rate = 1e-3\n",
        "hidden_dim = 128\n",
        "threshold = 0.5\n",
        "drop_prob = 0.6\n",
        "\n",
        "all_bce_losses = []\n",
        "all_hamm_losses = []\n",
        "all_jacc_scores = []\n",
        "all_lraps = []\n",
        "all_f1_macro = []\n",
        "all_f1_micro = []\n",
        "all_models = []\n",
        "\n",
        "for exp in range(1, 11):\n",
        "\n",
        "    print(\"Experiment {}\".format(exp), '\\n\\n')\n",
        "\n",
        "    model = CustomLSTM(embedding_dim = vocab.vectors.shape[1], vocab = vocab, hidden_dim = hidden_dim, output_dim = 11, drop_prob = drop_prob)\n",
        "    model = model.to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
        "    loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    train_loss, hamm_loss, jacc_score, lrap, f1_macro, f1_micro = train(model, loss_fn, optimizer, 50, train_dl, threshold)\n",
        "    torch.save(model, 'model{}.pth'.format(exp))\n",
        "    '''\n",
        "    print(\"Learning Rate : {}, Hidden Dimension : {}, Threshold : {}, Dropout Probability = {}\".format(learning_rate, hidden_dim, threshold, drop_prob))\n",
        "    print(\"Binary Cross Entropy With Logits Loss: {:.4f}\".format(min(train_loss)))\n",
        "    print(\"Hamming Loss : {:.4f}\".format(min(hamm_loss)))\n",
        "    print(\"Jaccard Score: {:.4f}\".format(max(jacc_score)))\n",
        "    print(\"Label Ranking Average Precision Score: {:.4f}\".format(max(lrap)))\n",
        "    print(\"F1 Macro Score: {:.4f}\".format(f1_macro))\n",
        "    print(\"F1 Micro Score: {:.4f}\".format(f1_micro))\n",
        "    print(\"\\n\")\n",
        "    '''\n",
        "\n",
        "    test_loss, test_hamm_loss, test_jacc_score, test_lrap, test_f1_macro, test_f1_micro = test(model, loss_fn, test_dl, threshold)\n",
        "    all_bce_losses.append(test_loss)\n",
        "    all_hamm_losses.append(test_hamm_loss)\n",
        "    all_jacc_scores.append(test_jacc_score)\n",
        "    all_lraps.append(test_lrap)\n",
        "    all_f1_macro.append(test_f1_macro)\n",
        "    all_f1_micro.append(test_f1_micro)\n",
        "\n",
        "\n",
        "    print(\"TestBinary Cross Entropy With Logits Loss: {:.4f}\".format(test_loss))\n",
        "    print(\"Test Hamming Loss : {:.4f}\".format(test_hamm_loss))\n",
        "    print(\"Test Jaccard Score: {:.4f}\".format(test_jacc_score))\n",
        "    print(\"Test Label Ranking Average Precision Score: {:.4f}\".format(test_lrap))\n",
        "    print(\"Test F1 Macro Score: {:.4f}\".format(test_f1_macro))\n",
        "    print(\"Test F1 Micro Score: {:.4f}\".format(test_f1_micro))\n",
        "    print(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yTHqv6lCKUW"
      },
      "source": [
        "print(\"Average Binary Cross Entropy With Logits Loss: {:.4f}\".format(sum(all_bce_losses)/10))\n",
        "print(\"Average Hamming Loss : {:.4f}\".format(sum(all_hamm_losses)/10))\n",
        "print(\"Average Jaccard Score: {:.4f}\".format(sum(all_jacc_scores)/10))\n",
        "print(\"Average Label Ranking Average Precision Score: {:.4f}\".format(sum(all_lraps)/10))\n",
        "print(\"Average F1 Macro Score: {:.4f}\".format(sum(all_f1_macro)/10))\n",
        "print(\"Average F1 Micro Score: {:.4f}\".format(sum(all_f1_micro)/10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nvm9AvjUdDl5"
      },
      "source": [
        "## Comparison between having GloVe Vectors and not having GloVe Vectors \n",
        "28-12-2020"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rwd0zhrTyM0W"
      },
      "source": [
        "%%time\n",
        "learning_rate = 1e-3\n",
        "hidden_dim = 128\n",
        "threshold = 0.5\n",
        "drop_prob = 0.65\n",
        "\n",
        "all_glove_losses = []\n",
        "all_glove_ham = []\n",
        "all_glove_jacc = []\n",
        "all_glove_lrap = []\n",
        "all_glove_f1mac = []\n",
        "all_glove_f1mic = []\n",
        "\n",
        "all_nonglove_losses = []\n",
        "all_nonglove_ham = []\n",
        "all_nonglove_jacc = []\n",
        "all_nonglove_lrap = []\n",
        "all_nonglove_f1mac = []\n",
        "all_nonglove_f1mic = []\n",
        "\n",
        "glove = [True, False]\n",
        "for exp in range(1, 11):\n",
        "    for g in glove:\n",
        "        model = CustomLSTM(embedding_dim = vocab.vectors.shape[1], vocab = vocab, hidden_dim = hidden_dim, output_dim = 11, drop_prob = drop_prob, use_glove = g)\n",
        "        model = model.to(device)\n",
        "\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
        "        loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "        train_loss, hamm_loss, jacc_score, lrap, f1_macro, f1_micro = train(model, loss_fn, optimizer, 50, train_dl, threshold)\n",
        "        test_loss, test_hamm_loss, test_jacc_score, test_lrap, test_f1_macro, test_f1_micro = test(model, loss_fn, test_dl, threshold)\n",
        "        print(\"Test Binary Cross Entropy With Logits Loss: {:.4f}\".format(test_loss))\n",
        "        print(\"Test Hamming Loss : {:.4f}\".format(test_hamm_loss))\n",
        "        print(\"Test Jaccard Score: {:.4f}\".format(test_jacc_score))\n",
        "        print(\"Test Label Ranking Average Precision Score: {:.4f}\".format(test_lrap))\n",
        "        print(\"Test F1 Macro Score: {:.4f}\".format(test_f1_macro))\n",
        "        print(\"Test F1 Micro Score: {:.4f}\".format(test_f1_micro))\n",
        "        print(\"\\n\")\n",
        "\n",
        "        if g is True:\n",
        "            all_glove_losses.append(test_loss)\n",
        "            all_glove_ham.append(test_hamm_loss)\n",
        "            all_glove_jacc.append(test_jacc_score)\n",
        "            all_glove_lrap.append(test_lrap)\n",
        "            all_glove_f1mac.append(test_f1_macro)\n",
        "            all_glove_f1mic.append(test_f1_micro)\n",
        "        else:\n",
        "            all_nonglove_losses.append(test_loss)\n",
        "            all_nonglove_ham.append(test_hamm_loss)\n",
        "            all_nonglove_jacc.append(test_jacc_score)\n",
        "            all_nonglove_lrap.append(test_lrap)\n",
        "            all_nonglove_f1mac.append(test_f1_macro)\n",
        "            all_nonglove_f1mic.append(test_f1_micro)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrqGW2V9EBfA"
      },
      "source": [
        "print(\"Using glove vectors:\")\n",
        "print(\"Average Binary Cross Entropy With Logits Loss: {:.4f}\".format(sum(all_glove_losses)/10))\n",
        "print(\"Average Hamming Loss : {:.4f}\".format(sum(all_glove_ham)/10))\n",
        "print(\"Average Jaccard Score: {:.4f}\".format(sum(all_glove_jacc)/10))\n",
        "print(\"Average Label Ranking Average Precision Score: {:.4f}\".format(sum(all_glove_lrap)/10))\n",
        "print(\"Average F1 Macro Score: {:.4f}\".format(sum(all_glove_f1mac)/10))\n",
        "print(\"Average F1 Micro Score: {:.4f}\".format(sum(all_glove_f1mic)/10))\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "print(\"Not using glove vectors:\")\n",
        "print(\"Average Binary Cross Entropy With Logits Loss: {:.4f}\".format(sum(all_nonglove_losses)/10))\n",
        "print(\"Average Hamming Loss : {:.4f}\".format(sum(all_nonglove_ham)/10))\n",
        "print(\"Average Jaccard Score: {:.4f}\".format(sum(all_nonglove_jacc)/10))\n",
        "print(\"Average Label Ranking Average Precision Score: {:.4f}\".format(sum(all_nonglove_lrap)/10))\n",
        "print(\"Average F1 Macro Score: {:.4f}\".format(sum(all_nonglove_f1mac)/10))\n",
        "print(\"Average F1 Micro Score: {:.4f}\".format(sum(all_nonglove_f1mic)/10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJOaX46ZaNuc"
      },
      "source": [
        "print(np.std(np.array(all_glove_losses)), np.std(np.array(all_glove_ham)), np.std(np.array(all_glove_jacc)), np.std(np.array(all_glove_lrap)),\n",
        "      np.std(np.array(all_glove_f1mac)), np.std(np.array(all_glove_f1mic)))\n",
        "\n",
        "print(np.std(np.array(all_nonglove_losses)), np.std(np.array(all_nonglove_ham)), np.std(np.array(all_nonglove_jacc)), np.std(np.array(all_nonglove_lrap)),\n",
        "      np.std(np.array(all_nonglove_f1mac)), np.std(np.array(all_nonglove_f1mic)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrJEnOMeghsr"
      },
      "source": [
        "Here we see that having pretrained word vectors helps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vs-V8hnPiA9h"
      },
      "source": [
        "## Comparing a unidirectional and a bidirectional network\n",
        "03-01-2021\n",
        "\n",
        "update - 10-01-2021"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OxY-zFbg9-u"
      },
      "source": [
        "learning_rate = 1e-3\n",
        "hidden_dim = 128\n",
        "threshold = 0.5\n",
        "drop_prob = 0.65\n",
        "\n",
        "iter = 1\n",
        "bce_losses = []\n",
        "ham_losses = []\n",
        "jacc_scores = []\n",
        "lrap_scores = []\n",
        "f1_macros = []\n",
        "f1_micros = []\n",
        "\n",
        "bi_bce_losses = []\n",
        "bi_ham_losses = []\n",
        "bi_jacc_scores = []\n",
        "bi_lrap_scores = []\n",
        "bi_f1_macros = []\n",
        "bi_f1_micros = []\n",
        "\n",
        "for exp in range(1, 11):\n",
        "    for dirn in [True, False]:\n",
        "        model = CustomLSTM(embedding_dim = vocab.vectors.shape[1], vocab = vocab, hidden_dim = hidden_dim, output_dim = 11, \n",
        "                            drop_prob = drop_prob, bidirectional = dirn, use_glove = True)\n",
        "        model = model.to(device)\n",
        "\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
        "        loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "        train_loss, hamm_loss, jacc_score, lrap, f1_macro, f1_micro = train(model, loss_fn, optimizer, 50, train_dl, threshold)\n",
        "        test_loss, test_hamm_loss, test_jacc_score, test_lrap, test_f1_macro, test_f1_micro = test(model, loss_fn, test_dl, threshold)\n",
        "        \n",
        "        if dirn is True:\n",
        "            bi_bce_losses.append(test_loss)\n",
        "            bi_ham_losses.append(test_hamm_loss)\n",
        "            bi_jacc_scores.append(test_jacc_score)\n",
        "            bi_lrap_scores.append(test_lrap)\n",
        "            bi_f1_macros.append(test_f1_macro)\n",
        "            bi_f1_micros.append(test_f1_micro)\n",
        "        else:\n",
        "            bce_losses.append(test_loss)\n",
        "            ham_losses.append(test_hamm_loss)\n",
        "            jacc_scores.append(test_jacc_score)\n",
        "            lrap_scores.append(test_lrap)\n",
        "            f1_macros.append(test_f1_macro)\n",
        "            f1_micros.append(test_f1_micro)\n",
        "\n",
        "        print(\"Iteration {} / {} done\".format(iter, 20))\n",
        "        iter += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmuZn0SORSfZ"
      },
      "source": [
        "print(\"Using bidirectional lstm:\")\n",
        "print(\"Average Binary Cross Entropy With Logits Loss: {:.4f}\".format(sum(bi_bce_losses)/10))\n",
        "print(\"Average Hamming Loss : {:.4f}\".format(sum(bi_ham_losses)/10))\n",
        "print(\"Average Jaccard Score: {:.4f}\".format(sum(bi_jacc_scores)/10))\n",
        "print(\"Average Label Ranking Average Precision Score: {:.4f}\".format(sum(bi_lrap_scores)/10))\n",
        "print(\"Average F1 Macro Score: {:.4f}\".format(sum(bi_f1_macros)/10))\n",
        "print(\"Average F1 Micro Score: {:.4f}\".format(sum(bi_f1_micros)/10))\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "print(\"Using lstm:\")\n",
        "print(\"Average Binary Cross Entropy With Logits Loss: {:.4f}\".format(sum(bce_losses)/10))\n",
        "print(\"Average Hamming Loss : {:.4f}\".format(sum(ham_losses)/10))\n",
        "print(\"Average Jaccard Score: {:.4f}\".format(sum(jacc_scores)/10))\n",
        "print(\"Average Label Ranking Average Precision Score: {:.4f}\".format(sum(lrap_scores)/10))\n",
        "print(\"Average F1 Macro Score: {:.4f}\".format(sum(f1_macros)/10))\n",
        "print(\"Average F1 Micro Score: {:.4f}\".format(sum(f1_micros)/10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UV0PyySgE4Q"
      },
      "source": [
        "print(\"Using bidirectional lstm:\")\n",
        "print(\"Deviation of Binary Cross Entropy With Logits Loss: {:.4f}\".format(np.std(np.array(bi_bce_losses))))\n",
        "print(\"Deviation of Hamming Loss : {:.4f}\".format(np.std(np.array(bi_ham_losses))))\n",
        "print(\"Deviation of Jaccard Score: {:.4f}\".format(np.std(np.array(bi_jacc_scores))))\n",
        "print(\"Deviation of Label Ranking Average Precision Score: {:.4f}\".format(np.std(np.array(bi_lrap_scores))))\n",
        "print(\"Deviation of F1 Macro Score: {:.4f}\".format(np.std(np.array(bi_f1_macros))))\n",
        "print(\"Deviation of F1 Micro Score: {:.4f}\".format(np.std(np.array(bi_f1_micros))))\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "print(\"Using lstm:\")\n",
        "print(\"Deviation of Binary Cross Entropy With Logits Loss: {:.4f}\".format(np.std(np.array(bce_losses))))\n",
        "print(\"Deviation of Hamming Loss : {:.4f}\".format(np.std(np.array(ham_losses))))\n",
        "print(\"Deviation of Jaccard Score: {:.4f}\".format(np.std(np.array(jacc_scores))))\n",
        "print(\"Deviation of Label Ranking Average Precision Score: {:.4f}\".format(np.std(np.array(lrap_scores))))\n",
        "print(\"Deviation of F1 Macro Score: {:.4f}\".format(np.std(np.array(f1_macros))))\n",
        "print(\"Deviation of F1 Micro Score: {:.4f}\".format(np.std(np.array(f1_micros))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LX7dndlfnQrD"
      },
      "source": [
        "we see that a bidirectional network performs much worse than a unidirectional network with the same set of parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zq0-UjGljpmd"
      },
      "source": [
        "## Comparing dropout values of 0, 0.5, 0.6, 0.7 for adding in the table\n",
        "06-01-2021\n",
        "\n",
        "update - 10-01-2021"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Q44D6Hqjwp1"
      },
      "source": [
        "learning_rates = [1e-3]\n",
        "hidden_dims = [128]\n",
        "drop_probs = [0, 0.5, 0.6, 0.7]\n",
        "\n",
        "for drop_prob in drop_probs:\n",
        "    bce_losses = []\n",
        "    ham_losses = []\n",
        "    jacc_scores = []\n",
        "    lrap_scores = []\n",
        "    f1_macros = []\n",
        "    f1_micros = []\n",
        "    for exp in range(1, 11):\n",
        "        model = CustomLSTM(embedding_dim = vocab.vectors.shape[1], vocab = vocab, hidden_dim = hidden_dim, output_dim = 11, drop_prob = drop_prob)\n",
        "        model = model.to(device)\n",
        "\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
        "        loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "        train_loss, hamm_loss, jacc_score, lrap, f1_macro, f1_micro = train(model, loss_fn, optimizer, 50, train_dl, threshold)\n",
        "        #all_models.append(model)\n",
        "\n",
        "        test_loss, test_hamm_loss, test_jacc_score, test_lrap, test_f1_macro, test_f1_micro = test(model, loss_fn, test_dl, threshold)\n",
        "        bce_losses.append(test_loss)\n",
        "        ham_losses.append(test_hamm_loss)\n",
        "        jacc_scores.append(test_jacc_score)\n",
        "        lrap_scores.append(test_lrap)\n",
        "        f1_macros.append(test_f1_macro)\n",
        "        f1_micros.append(test_f1_micro)\n",
        "    \n",
        "    print(\"Dropout = {}\".format(drop_prob))\n",
        "    print(\"Average Binary Cross Entropy With Logits Loss: {:.4f}\".format(sum(bce_losses)/10))\n",
        "    print(\"Average Hamming Loss : {:.4f}\".format(sum(ham_losses)/10))\n",
        "    print(\"Average Jaccard Score: {:.4f}\".format(sum(jacc_scores)/10))\n",
        "    print(\"Average Label Ranking Average Precision Score: {:.4f}\".format(sum(lrap_scores)/10))\n",
        "    print(\"Average F1 Macro Score: {:.4f}\".format(sum(f1_macros)/10))\n",
        "    print(\"Average F1 Micro Score: {:.4f}\".format(sum(f1_micros)/10))\n",
        "    print(\"\\n\\n\")\n",
        "\n",
        "    print(\"Deviation of Binary Cross Entropy With Logits Loss: {:.4f}\".format(np.std(np.array(bce_losses))))\n",
        "    print(\"Deviation of Hamming Loss : {:.4f}\".format(np.std(np.array(ham_losses))))\n",
        "    print(\"Deviation of Jaccard Score: {:.4f}\".format(np.std(np.array(jacc_scores))))\n",
        "    print(\"Deviation of Label Ranking Average Precision Score: {:.4f}\".format(np.std(np.array(lrap_scores))))\n",
        "    print(\"Deviation of F1 Macro Score: {:.4f}\".format(np.std(np.array(f1_macros))))\n",
        "    print(\"Deviation of F1 Micro Score: {:.4f}\".format(np.std(np.array(f1_micros))))\n",
        "    print(\"\\n\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgYWErjDC_sS"
      },
      "source": [
        "print(\"BCE Losses: \", bce_losses)\n",
        "print(\"Hamming Losses: \", ham_losses)\n",
        "print(\"Jaccard Scores: \", jacc_scores)\n",
        "print(\"LRAP Scores: \", lrap_scores)\n",
        "print(\"F1 Macros: \", f1_macros)\n",
        "print(\"F1 Micros: \", f1_micros)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlyFiw4A7a1H"
      },
      "source": [
        "## Modeling on IEEE Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvgrA7LA7d0L"
      },
      "source": [
        "ieee = pd.read_csv(\"/content/drive/MyDrive/Covid 19 India/ieee_extract.csv\")\n",
        "ieee['tweet'] = ieee['tweet'].str.lower()\n",
        "ieee.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fqg-UF1JPAyp"
      },
      "source": [
        "def emoji2text(tweet):\n",
        "    emojis = demoji.findall(tweet)\n",
        "    new_tweet = []\n",
        "    for word in tweet.split():\n",
        "        if word in emojis.keys():\n",
        "            tweet = tweet.replace(word, emojis[word])\n",
        "            new_tweet.append(emojis[word])\n",
        "        wordmojis = demoji.findall(word)\n",
        "        for char in word:\n",
        "            if char in wordmojis.keys():\n",
        "                tweet = tweet.replace(word, wordmojis[char])\n",
        "    \n",
        "    return tweet\n",
        "\n",
        "def remove_hashtags(tweet):\n",
        "    return re.sub(r'\\#w+', '', tweet)\n",
        "\n",
        "def remove_mentions(tweet):\n",
        "    for word in tweet.split():\n",
        "        if word[0] == '@':\n",
        "            tweet = tweet.replace(word, '')\n",
        "    return tweet\n",
        "\n",
        "def remove_punctuations(tweet):\n",
        "    punct = string.punctuation\n",
        "    trantab = str.maketrans(punct, len(punct)*' ')\n",
        "    return tweet.translate(trantab)\n",
        "\n",
        "def remove_url(text):\n",
        "    return re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', text, flags = re.MULTILINE)\n",
        "\n",
        "def clean(tweet):\n",
        "    tweet = emoji2text(tweet)\n",
        "    tweet = remove_hashtags(tweet)\n",
        "    tweet = remove_mentions(tweet)\n",
        "    #tweet = remove_punctuations(tweet)\n",
        "    tweet = remove_url(tweet)\n",
        "    return tweet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMinjh7yerv_"
      },
      "source": [
        "ieee['tweet'] = ieee['tweet'].astype(str)\n",
        "ieee['tweet'] = ieee['tweet'].astype(str)\n",
        "ieee['tweet'] = ieee['tweet'].astype(str)\n",
        "ieee['tweet'] = ieee['tweet'].apply(lambda x : clean(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uiTk7-MSgDg"
      },
      "source": [
        "ieee"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08hpyo0OD6PM"
      },
      "source": [
        "ieee = ieee.dropna()\n",
        "ieee = ieee.drop_duplicates(['tweet'])\n",
        "ieee['tweet'] = ieee['tweet'].str.lower()\n",
        "ieee['month'] = ieee['month'].str.lower()\n",
        "ieee['place'] = ieee['place'].str.lower()\n",
        "ieee['tweet'] = ieee['tweet'].apply(lambda x : re.sub('\\n', '', x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZ6I-HBPE17d"
      },
      "source": [
        "ieee = ieee.reset_index()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkM0pPrKaAKD"
      },
      "source": [
        "# **Loading Bert**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjSm0e_ymV-p"
      },
      "source": [
        "bert = torch.load(\"/content/drive/MyDrive/Covid 19 India/bertmodel.pth\")\n",
        "bert"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUPKp8GIFSmr"
      },
      "source": [
        "bert_df = pd.DataFrame()\n",
        "bert_df['Tweet'] = ieee['tweet']\n",
        "values = [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]] * 145556\n",
        "bert_df['list'] = values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNFO3iihGqNS"
      },
      "source": [
        "test_dataset = CustomDataset(bert_df, tokenizer, MAX_LEN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCIKngPrHD5p"
      },
      "source": [
        "bert_test_params = {'batch_size': 1,\n",
        "                    'shuffle': False,\n",
        "                    'num_workers': 0\n",
        "                    }   \n",
        "\n",
        "test_loader = DataLoader(test_dataset, **bert_test_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hD-K_pIFHjap"
      },
      "source": [
        "def test():\n",
        "    bert.eval()\n",
        "    bert_outputs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for unw, data in enumerate(test_loader, 0):\n",
        "            ids = data['ids'].to(device, dtype = torch.long)\n",
        "            mask = data['mask'].to(device, dtype = torch.long)\n",
        "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "            targets = data['targets'].to(device, dtype = torch.float)\n",
        "            outputs = bert(ids, mask, token_type_ids)\n",
        "\n",
        "            bert_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
        "\n",
        "    return bert_outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mABiOSX0IKe4"
      },
      "source": [
        "test_outputs = test()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c075oiKgZ8TQ"
      },
      "source": [
        "test_outputs = np.array(test_outputs)\n",
        "\n",
        "for i in range(test_outputs.shape[0]):\n",
        "    for j in range(test_outputs.shape[1]):\n",
        "        if test_outputs[i][j] >= 0.5: test_outputs[i][j] = 1\n",
        "        else: test_outputs[i][j] = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5ZU-YVyuH3L"
      },
      "source": [
        "bert_df['Optimistic'] = \"None\"\n",
        "bert_df['Thankful'] = \"None\"\n",
        "bert_df['Empathetic'] = \"None\"\n",
        "bert_df['Pessimistic'] = \"None\"\n",
        "bert_df['Anxious'] = \"None\"\n",
        "bert_df['Sad'] = \"None\"\n",
        "bert_df['Annoyed'] = \"None\"\n",
        "bert_df['Denial'] = \"None\"\n",
        "bert_df['Official report'] = \"None\"\n",
        "bert_df['Surprise'] = \"None\"\n",
        "bert_df['Joking'] = \"None\"\n",
        "bert_df = bert_df.drop(['list'], axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okky0Xy4voRD"
      },
      "source": [
        "for i in range(len(test_outputs)):\n",
        "    bert_df['Optimistic'].iloc[i] = test_outputs[i][0]\n",
        "    bert_df['Thankful'].iloc[i] = test_outputs[i][1]\n",
        "    bert_df['Empathetic'].iloc[i] = test_outputs[i][2]\n",
        "    bert_df['Pessimistic'].iloc[i] = test_outputs[i][3]\n",
        "    bert_df['Anxious'].iloc[i] = test_outputs[i][4]\n",
        "    bert_df['Sad'].iloc[i] = test_outputs[i][5]\n",
        "    bert_df['Annoyed'].iloc[i] = test_outputs[i][6]\n",
        "    bert_df['Denial'].iloc[i] = test_outputs[i][7]\n",
        "    bert_df['Official report'].iloc[i] = test_outputs[i][8]\n",
        "    bert_df['Surprise'].iloc[i] = test_outputs[i][9]\n",
        "    bert_df['Joking'].iloc[i] = test_outputs[i][10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOCRPqLd3Mkd"
      },
      "source": [
        "bert_df['month'] = ieee['month']\n",
        "bert_df['place'] = ieee['place']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJ6Cq7q524A8"
      },
      "source": [
        "bert_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WMTr98i3jOR"
      },
      "source": [
        "bert_df.to_csv(\"bert_final.csv\", index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPVy2aIZaKuh"
      },
      "source": [
        "# **Loading LSTM**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEu9g-bQ8MKz"
      },
      "source": [
        "model = torch.load(\"/content/drive/MyDrive/Covid 19 India/model7LSTM.pth\")\n",
        "model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-18IyiynR7d"
      },
      "source": [
        "ieee = ieee.drop(['sentiment_score'], axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOyy6q3CLeB2"
      },
      "source": [
        "import spacy\n",
        "spacy_en = spacy.load('en')\n",
        "\n",
        "def tokenizer(tweet):\n",
        "    tweet = re.sub(r'[\\n]', ' ', tweet)\n",
        "    return [tok.text for tok in spacy_en.tokenizer(tweet)]\n",
        "\n",
        "tweet_field = data.Field(sequential = True, lower = True, tokenize = tokenizer)\n",
        "\n",
        "dataFields = [(\"tweet\", tweet_field), (\"sentiment_score\", None), (\"place\", None), (\"month\", None)]\n",
        "\n",
        "train_dataset = data.TabularDataset(\n",
        "    path = '/content/final.csv', format = 'csv', fields = dataFields, skip_header = True\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCK_svhOL1do"
      },
      "source": [
        "print(\"length of dataset:\", len(train_dataset))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzRrbeQcMAVH"
      },
      "source": [
        "tweet_field.vocab = TWEET.vocab\n",
        "BATCH_SIZE = 1\n",
        "n_label = 11\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iter = data.BucketIterator(train_dataset,\n",
        "                                batch_size = BATCH_SIZE,\n",
        "                                sort_key = lambda x : len(x.tweet),\n",
        "                                sort_within_batch = False,\n",
        "                                repeat = False,\n",
        "                                shuffle = False,\n",
        "                                device = device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAWvFeKGMZ8s"
      },
      "source": [
        "train_dl = BatchWrapper(train_iter, \"tweet\", None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sD01UpMragUG"
      },
      "source": [
        "i = 0\n",
        "for x, y in train_dl:\n",
        "    print(x.squeeze())\n",
        "    i += 1\n",
        "    if i == 10: break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2qky3wAMvMO"
      },
      "source": [
        "predicted_labels = []\n",
        "sus = [33484, 62825]\n",
        "i = 0\n",
        "for X, y in train_dl:\n",
        "    i += 1\n",
        "    if i not in sus:\n",
        "        preds = model(X)\n",
        "        sig = nn.Sigmoid()\n",
        "        out = sig(preds)\n",
        "\n",
        "        out[out >= 0.5] = 1\n",
        "        out[out < 0.5] = 0\n",
        "        predicted_labels.append(out.detach().cpu().numpy().tolist())\n",
        "        if i % 100 == 0:\n",
        "            print(\"{}/{} iterations done\".format(i, len(train_dl)))\n",
        "    else:\n",
        "        predicted_labels.append([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJ0nb-FnnxWF"
      },
      "source": [
        "len(predicted_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLjhfn1OwckZ"
      },
      "source": [
        "till_now = predicted_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOMsDT3VvpO5"
      },
      "source": [
        "sorted(soi['month'].value_counts())\n",
        "#sep, apr, mar, jun, may, aug, jul"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dziIgS5bvz-M"
      },
      "source": [
        "soi['month'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vauBpQ7wA88"
      },
      "source": [
        "soi['Optimistic'] = \"None\"\n",
        "soi['Thankful'] = \"None\"\n",
        "soi['Empathetic'] = \"None\"\n",
        "soi['Pessimistic'] = \"None\"\n",
        "soi['Anxious'] = \"None\"\n",
        "soi['Sad'] = \"None\"\n",
        "soi['Annoyed'] = \"None\"\n",
        "soi['Denial'] = \"None\"\n",
        "soi['Official report'] = \"None\"\n",
        "soi['Surprise'] = \"None\"\n",
        "soi['Joking'] = \"None\"\n",
        "soi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTceaw9Uw8y9"
      },
      "source": [
        "for i in range(len(till_now)):\n",
        "    soi['Optimistic'].iloc[i] = till_now[i][0]\n",
        "    soi['Thankful'].iloc[i] = till_now[i][1]\n",
        "    soi['Empathetic'].iloc[i] = till_now[i][2]\n",
        "    soi['Pessimistic'].iloc[i] = till_now[i][3]\n",
        "    soi['Anxious'].iloc[i] = till_now[i][4]\n",
        "    soi['Sad'].iloc[i] = till_now[i][5]\n",
        "    soi['Annoyed'].iloc[i] = till_now[i][6]\n",
        "    soi['Denial'].iloc[i] = till_now[i][7]\n",
        "    soi['Official report'].iloc[i] = till_now[i][8]\n",
        "    soi['Surprise'].iloc[i] = till_now[i][9]\n",
        "    soi['Joking'].iloc[i] = till_now[i][10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcXg05F7yEiv"
      },
      "source": [
        "soi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msm1xull7V93"
      },
      "source": [
        "soi.to_csv(\"finaldf.csv\", index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuVf8c7eyNcN"
      },
      "source": [
        "rand_idx = np.random.randint(0, 33483)\n",
        "print(rand_idx)\n",
        "print(soi.iloc[rand_idx])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHm83I_r0PP_"
      },
      "source": [
        "soi['tweet'].iloc[rand_idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXshGDFK0XOs"
      },
      "source": [
        "rand_idx = np.random.randint(0, 33483)\n",
        "print(rand_idx)\n",
        "print(soi.iloc[rand_idx])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dP2cuw880sO5"
      },
      "source": [
        "soi['tweet'].iloc[rand_idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKDHS71w1yr7"
      },
      "source": [
        "rand_idx = np.random.randint(0, 33483)\n",
        "print(rand_idx)\n",
        "print(soi.iloc[rand_idx])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7o8Zeacd10VC"
      },
      "source": [
        "soi['tweet'].iloc[rand_idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIm8loLy2F5N"
      },
      "source": [
        "## **Visualisations on the first 33k examples**\n",
        "last edit - 28-01-2021"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USuRRjR91FLq"
      },
      "source": [
        "req = soi.iloc[:33483]\n",
        "req"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTHi5RX-2SBE"
      },
      "source": [
        "values = []\n",
        "for col in req.columns[4:]:\n",
        "    values.append([req[col].value_counts()[0], req[col].value_counts()[1]])\n",
        "\n",
        "labels = req.columns[4:]\n",
        "x = np.arange(len(labels))\n",
        "width = 0.35\n",
        "\n",
        "zero_means = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "one_means = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "plt.figure(figsize = (14, 14))\n",
        "rects1 = ax.bar(x - width/2, zero_means, width, label = 'Zeros')\n",
        "rects2 = ax.bar(x + width/2, one_means, width, label = 'Ones') \n",
        "\n",
        "ax.set_ylabel('Frequency')\n",
        "#ax.set_title('')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(labels)\n",
        "ax.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edRImnYk3mQ4"
      },
      "source": [
        "values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAB21ICV5JRl"
      },
      "source": [
        "plt.subplots?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULbIcSSL6YbE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}